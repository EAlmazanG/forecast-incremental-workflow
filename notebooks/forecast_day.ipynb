{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, kpss, acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n",
    "from src import validation\n",
    "from src import base_forecast\n",
    "from src.utils import print_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)\n",
    "importlib.reload(validation)\n",
    "importlib.reload(base_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA INGESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_df_raw = pd.read_csv('../data/day.csv')\n",
    "display(day_df_raw.head())\n",
    "print('DF Size: ', day_df_raw.shape)\n",
    "print('DF Types: \\n', day_df_raw.dtypes)\n",
    "\n",
    "df = day_df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_columns = ['dteday']\n",
    "float64_columns = ['temp','atemp','hum','windspeed']\n",
    "str_columns = ['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "int64_columns = ['casual', 'registered', 'cnt']\n",
    "\n",
    "# Basic data conversion\n",
    "df = utils.format_columns(df, datetime_columns, int64_columns, float64_columns, str_columns)\n",
    "# Rename columns\n",
    "df.rename(columns={\n",
    "    'dteday':'date',\n",
    "    'yr':'year',\n",
    "    'mnth':'month',\n",
    "    'weathersit':'weather',\n",
    "    'temp':'temperature',\n",
    "    'atemp':'temperature_sensation',\n",
    "    'hum':'humidity',\n",
    "    'casual':'casual_users',\n",
    "    'registered':'registered_users',\n",
    "    'cnt':'bikes_rented'\n",
    "}, inplace=True)\n",
    "# Drop not used columns\n",
    "df.drop(columns=['instant'], inplace=True)\n",
    "\n",
    "# Check dtypes\n",
    "print_title('CONVERTED DATA TYPES')\n",
    "print(df.dtypes)\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA CLEANING AND QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DESCRIBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick checks on data\n",
    "print_title('DF INFO')\n",
    "display(df.info())\n",
    "\n",
    "print_title('DF DESCRIBE')\n",
    "display(df.describe())\n",
    "\n",
    "# Check distribution of variants\n",
    "print_title('DISTRIBUTIONS')\n",
    "display(df['season'].value_counts().sort_index())\n",
    "display(df['year'].value_counts().sort_index())\n",
    "display(df['month'].value_counts().sort_index())\n",
    "display(df['holiday'].value_counts().sort_index())\n",
    "display(df['weekday'].value_counts().sort_index())\n",
    "display(df['workingday'].value_counts().sort_index())\n",
    "display(df['weather'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DUPLICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for and drop duplicates in the entire DataFrame\n",
    "duplicated_rows = df.duplicated().sum()\n",
    "print('# of duplicated rows: ', duplicated_rows)\n",
    "\n",
    "if duplicated_rows > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print('Duplicates in the DataFrame removed.')\n",
    "else:\n",
    "    print('No duplicates in the DataFrame found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "primary_key_column = 'date'\n",
    "\n",
    "# Check for duplicates in the unique columns\n",
    "duplicated_rows = df[df[primary_key_column].duplicated(keep=False)]\n",
    "print(f'# of duplicated on {primary_key_column} column: {duplicated_rows[primary_key_column].nunique()}')\n",
    "\n",
    "if not duplicated_rows.empty:\n",
    "    print(f'Duplicated {primary_key_column} and their rows:')\n",
    "    display(duplicated_rows.sort_values(by = primary_key_column))\n",
    "\n",
    "    # Keep only the first following timestamp column order\n",
    "    if primary_key_column == '':\n",
    "        df = df.drop_duplicates(subset=primary_key_column, keep='first')\n",
    "        print('Kept the most recent row for each duplicated' +  primary_key_column)\n",
    "    else:\n",
    "        df = df.sort_values(primary_key_column).drop_duplicates(subset=primary_key_column, keep='first')\n",
    "        print('Kept the most recent row for each duplicated ' + primary_key_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print_title('NUMBER OF NULL VALUES')\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUTLIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['casual_users', 'registered_users', 'bikes_rented']\n",
    "\n",
    "numeric_cols = df.drop(columns=target_columns, errors='ignore').select_dtypes(include=[\"number\"])\n",
    "outliers_df = utils.detect_outliers(numeric_cols, method=\"iqr\")\n",
    "outlier_rows = df.loc[outliers_df.any(axis=1)]\n",
    "print_title('ANOMALY ROWS')\n",
    "display(outliers_df[outliers_df['is_outlier']])\n",
    "\n",
    "df[\"extreme_weather\"] = outliers_df[\"is_outlier\"].astype(int).astype(str)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'bikes_rented'\n",
    "try:\n",
    "    df.drop(columns=['registered_users', 'casual_users'], inplace= True)\n",
    "except:\n",
    "    pass\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_time_series(df, [target_column], 0.9, '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STATIONARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stationary_result = adfuller(df[\"bikes_rented\"])\n",
    "is_stationary_resultt = True if stationary_result[1] < 0.05 else False\n",
    "print(f\"ADF Statistic: {stationary_result[0]}\")\n",
    "print(f\"P-value: {stationary_result[1]}\")\n",
    "print(\"Conclusion:\", \"Stationary\" if stationary_result[1] < 0.05 else \"Non-stationary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trend_result = kpss(df[\"bikes_rented\"], regression=\"c\")\n",
    "is_trend_present = True if trend_result[1] < 0.05 else False\n",
    "print(f\"KPSS Statistic: {trend_result[0]}\")\n",
    "print(f\"P-value: {trend_result[1]}\")\n",
    "print(\"Conclusion:\", \"Trend present\" if trend_result[1] < 0.05 else \"No significant trend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIME PERSISTENCE AND SEASONALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlags = 365\n",
    "acf_values = acf(df[\"bikes_rented\"], nlags=nlags, fft=True)\n",
    "\n",
    "autocorrelated_lags = np.where(np.abs(acf_values) > 0.5)[0]\n",
    "autocorrelation_ratio = len(autocorrelated_lags) / nlags\n",
    "\n",
    "seasonal_lags = [lag for lag in autocorrelated_lags if lag % 7 == 0 or lag % 30 == 0 or lag % 365 == 0]\n",
    "\n",
    "print(f\"Lags with autocorrelation: {autocorrelated_lags.tolist()}\")\n",
    "print(f\"Percentage of significant lags: {autocorrelation_ratio:.2%}\")\n",
    "print(f\"Possible seasonal lags: {seasonal_lags}\")\n",
    "\n",
    "if autocorrelation_ratio > 0.5:\n",
    "    print(\"Strong temporal dependence detected.\")\n",
    "elif autocorrelation_ratio > 0.3:\n",
    "    print(\"Moderate temporal dependence detected.\")\n",
    "else:\n",
    "    print(\"Low temporal dependence.\")\n",
    "\n",
    "if len(seasonal_lags) > 2:\n",
    "    is_seasonality_present = True\n",
    "    print(\"Seasonality detected.\")\n",
    "else:\n",
    "    is_seasonality_present = False\n",
    "    print(\"No clear seasonality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACF AND PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_acf_and_pacf(df[target_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECOMPOSITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed = seasonal_decompose(df[target_column], model='additive', period=30)\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 16))\n",
    "\n",
    "axes[0].plot(df[\"date\"], decomposed.observed, color=\"black\", linewidth=2)\n",
    "axes[0].set_title(\"Original\", fontsize=14, fontweight='bold')\n",
    "\n",
    "axes[1].plot(df[\"date\"], decomposed.trend, color=\"tab:blue\", linewidth=2)\n",
    "axes[1].set_title(\"Trend\", fontsize=14, fontweight='bold')\n",
    "\n",
    "axes[2].plot(df[\"date\"], decomposed.seasonal, color=\"tab:green\", linewidth=2)\n",
    "axes[2].set_title(\"Seasonality\", fontsize=14, fontweight='bold')\n",
    "\n",
    "axes[3].plot(df[\"date\"], decomposed.resid, color=\"tab:red\", linewidth=2)\n",
    "axes[3].set_title(\"Residuals\", fontsize=14, fontweight='bold')\n",
    "\n",
    "for ax in axes:\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=2))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
    "    ax.tick_params(axis='x', rotation=0, labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "utils.plot_acf_and_pacf(decomposed.resid.dropna(), ' - Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUTLIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,4))\n",
    "\n",
    "sns.boxplot(\n",
    "    x=df[target_column], \n",
    "    ax=ax, \n",
    "    flierprops={\"marker\": \"o\", \"markerfacecolor\": \"red\", \"markeredgecolor\": \"black\", \"markersize\": 6}\n",
    ")\n",
    "\n",
    "ax.set_title(f\"Outlier Detection in {target_column}\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlabel(target_column, fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXOGENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_columns = [\"bikes_rented\", \"date\"]\n",
    "\n",
    "exogen_columns = [\n",
    "    col for col in df.columns \n",
    "    if col not in excluded_columns and df[col].dtype in [\"int64\", \"float64\", \"uint8\", \"object\"]\n",
    "]\n",
    "print(\"Exogenous variables for forecasting:\", exogen_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUMERIC CORRELATION AND COLLINEARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 2))\n",
    "numeric_to_corr_df = df.select_dtypes(include=[\"number\"]).drop(columns=[target_column], errors=\"ignore\")\n",
    "corr_df = numeric_to_corr_df.corr()\n",
    "sns.heatmap(corr_df, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation between numeric variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numeric_to_corr_df.dropna().select_dtypes(include=[\"number\"]).astype(\"float64\")\n",
    "\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X.dropna(inplace=True)\n",
    "\n",
    "corr_matrix = X.corr().abs()\n",
    "high_corr_pairs = np.where(corr_matrix > 0.8)\n",
    "high_corr_pairs = [(corr_matrix.index[i], corr_matrix.columns[j]) \n",
    "                   for i, j in zip(*high_corr_pairs) if i != j and i < j]\n",
    "\n",
    "print(\"Highly correlated variable pairs:\", high_corr_pairs)\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif_data = vif_data.sort_values(by=\"VIF\", ascending=False)\n",
    "\n",
    "print('\\n')\n",
    "print(\"VIF:\")\n",
    "print(vif_data)\n",
    "\n",
    "target_corr = df.corr()[\"bikes_rented\"].drop([\"bikes_rented\", \"date\"])\n",
    "print('\\n')\n",
    "print(\"ABS Correlation values:\")\n",
    "print(abs(target_corr).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASE FORECAST FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_remove = set()\n",
    "for pair in high_corr_pairs:\n",
    "    feature_to_remove = pair[0] if abs(target_corr[pair[0]]) < abs(target_corr[pair[1]]) else pair[1]\n",
    "    features_to_remove.add(feature_to_remove)\n",
    "\n",
    "# Remove high VIF features (threshold >10)\n",
    "high_vif_features = vif_data[vif_data[\"VIF\"] > 10][\"Variable\"].tolist()\n",
    "features_to_remove.update(high_vif_features)\n",
    "\n",
    "# Select relevant features\n",
    "selected_features = [feature for feature in exogen_columns if feature not in features_to_remove]\n",
    "selected_features = sorted(selected_features, key=lambda x: abs(target_corr[x]), reverse=True)\n",
    "print(\"Final selected exogenous variables:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIME NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_time_normalization = False\n",
    "selection_df = df.copy()\n",
    "target_time_norm_column = target_column + \"_time_norm\"\n",
    "\n",
    "if use_time_normalization:\n",
    "    forecast_column = target_time_norm_column\n",
    "    selection_df[forecast_column] = df[target_column] / df[\"date\"].dt.days_in_month\n",
    "    try:\n",
    "        selection_df.drop(columns=target_column, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    forecast_column = target_column\n",
    "    try:\n",
    "        selection_df.drop(columns=target_time_norm_column, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print('Forecast variable: ', forecast_column)\n",
    "display(selection_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORMATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_df = selection_df[['date'] + selected_features + [forecast_column]]\n",
    "\n",
    "print_title('Recommended Transformations')\n",
    "transformations = utils.check_transformations(selection_df)\n",
    "\n",
    "print('\\n')\n",
    "print_title('DF Prepared')\n",
    "transformed_df = utils.apply_transformations(selection_df, transformations)\n",
    "print('Forecast variable: ', forecast_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE AND VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_forecast_df = base_forecast.naive_forecast(transformed_df, forecast_column, steps=60)\n",
    "baseline_column = 'naive_forecast'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.plot_time_series_forecast(naive_forecast_df, [forecast_column, 'naive_forecast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = validation.calculate_forecast_metrics(naive_forecast_df, naive_forecast_df, forecast_column, \"naive_forecast\")\n",
    "print(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIME VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk-Forward Validation\n",
    "walk_results = validation.walk_forward_validation(naive_forecast_df, naive_forecast_df, \"bikes_rented\", \"naive_forecast\", steps=30, n_splits=5)\n",
    "\n",
    "# Expanding Window Validation\n",
    "expanding_results = validation.expanding_window_validation(naive_forecast_df, naive_forecast_df, \"bikes_rented\", \"naive_forecast\", steps=30, initial_train_size=100)\n",
    "\n",
    "# Rolling Window Validation\n",
    "rolling_results = validation.rolling_window_validation(naive_forecast_df, naive_forecast_df, \"bikes_rented\", \"naive_forecast\", steps=30, window_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.plot_validation_results(walk_results, expanding_results, rolling_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESIDUALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.check_forecast_residuals(naive_forecast_df, \"bikes_rented\", \"naive_forecast\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_results = validation.compare_forecast_models(naive_forecast_df, naive_forecast_df, naive_forecast_df, forecast_column, \"naive_forecast\")\n",
    "formatted_results_df = validation.format_comparison_results(comparison_results)\n",
    "display(formatted_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASE FORECASTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOM WALK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_walk_drift = True if is_trend_present else False\n",
    "random_walk_forecast_df = base_forecast.random_walk_forecast(transformed_df, forecast_column, steps=60, drift=random_walk_drift)\n",
    "random_walk_forecast_metrics, random_walk_forecast_comparison_results = validation.validate_forecast(naive_df=naive_forecast_df, forecast_df=random_walk_forecast_df, baseline_df=naive_forecast_df, to_forecast_column = forecast_column, forecasted_column = 'random_walk_forecast')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPONENTIAL SMOOTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_smoothing_forecast_df = exponential_smoothing_forecast(transformed_df, forecast_column, steps=60)\n",
    "exponential_smoothing_walk_forecast_metrics, exponential_smoothing_comparison_results = validation.validate_forecast(naive_df=naive_forecast_df, forecast_df=exponential_smoothing_forecast_df, baseline_df=naive_forecast_df, to_forecast_column = forecast_column, forecasted_column = 'exponential_smoothing_forecast')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STATISTICAL FORECASTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML FORECAST FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMPLETE CORRELATION AND COLLINEARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 16))\n",
    "df_encoded_to_corr = pd.get_dummies(df.drop(columns=[target_column, \"date\"], errors=\"ignore\"), drop_first=True)\n",
    "df_corr = df_encoded_to_corr.corr()\n",
    "sns.heatmap(df_corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation between all variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
